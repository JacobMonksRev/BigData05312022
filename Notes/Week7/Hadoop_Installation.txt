### CHECKING JAVA

```sudo apt update```

//Check which version of java you have.
```java -version```

    - If 'Command not found',
        ```sudo apt install openjdk-8-jre-headless

//Install OpenJDK:
```sudo apt-get install openjdk-8-jdk```

### INSTALLING HADOOP 3.3.0
```wget http://mirror.intergrid.com.au/apache/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz```

//Make the hadoop directory:
```mkdir ~/hadoop```
```tar -xvzf hadoop-3.3.0.tar.gz -C ~/hadoop```

//Check that it was intalled in the hadoop folder:
```cd ~/hadoop/hadoop-3.3.0```

### CONFIGURE SSH
//Check if you can safely SSH to localhost:
```ssh localhost```

    // If you cannot ssh to localhost without a passphrase
    // or it says Permission Denied (publickey):
    ```ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa```
    ```cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys```
    ```chmod 0600 ~/.ssh/authorized_keys```

    // if 'Port 22: Connection Refused':
    ```sudo apt-get install ssh```
    ```sudo service ssh restart```

### SETUP ENVIRONMENT VARIABLES (optional but recommended)
```vi ~/.bashrc```
// Add the following lines to .bashrc:

export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
export HADOOP_HOME=~/hadoop/hadoop-3.3.0
export PATH=$PATH:$HADOOP_HOME/bin
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

// Change the java version if necessary.
// Exit file editor
:wq

```source ~/.bashrc```

```cd ~/hadoop/hadoop-3.3.0```
```vi etc/hadoop-env.sh```
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

```vi etc/hadoop/core-site.xml```

//Check the <configuration> tags and insert this code between them:
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>

```vi etc/hadoop/hdfs-site.xml```
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>

```vi etc/hadoop/mapred-site.xml```
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>

```vi etc/hadoop/yarn-site.xml```
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>

### FORMAT NAMENODE
// After making all the changes to the Environment Variables, format the name node:
```bin/hdfs namenode -format```

### RUN DFS DAEMONS
``` cd ~/hadoop/hadoop-3.3.0```
```sbin/start-dfs.sh```

// Should get messages like:
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes

// Check the running daemons with:
```jps```
2212 NameNode
2423 DataNode
2682 SecondaryNameNode
2829 Jps

        // IF DATANODE DOES NOT APPEAR AFTER 'jps' //
        ```cat /tmp/hadoop-(insert your linux name here)/dfs/data/current/VERSION```
        ```cat /tmp/hadoop-(insert your linux name here)/dfs/name/current/VERSION```

        // Make note of the two ClusterID fields in these files.
        // If they are different, copy the Namenode ClusterID and put it into the DataNode ClusterID.

// Start YARN with:
```sbin/start-yarn.sh```

// Stop daemons with:
```sbin/stop-all.sh```

// To enter Web Interface for Hadoop:
http://localhost:9870
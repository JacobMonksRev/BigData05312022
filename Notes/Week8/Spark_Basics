WHAT IS SPARK?

Spark is an open-source data processing engine.
It is a software utility of Hadoop.
It splits tasks acoss multiple nodes ina Cluster to perform parallel processing.
Spark uses RAM to process and cache data instead of the file system.
- this allows for faster access.
- memory is voltaile and not saved when the application stops.

Cluster Manager can be run in two different modes: Cluster Mode and Client Mode
- Client Mode: 
    Driver Program runs separate from the Cluster.
    This is what we use for single-node clusters.
- Cluster Mode: 
    Driver Program runs inside of a YARN container.

RDD - Resilient Distributed Dataset
- Primary API of Spark
- if the system crashes, the RDD will be maintained.
- RDD itself does not store any data.
    - the RDD has the information that is necssary for recreating the data.
    - an RDD Lineage refers to the steps taken in order to create the specific RDD.
- RDDs are lazy and ephemeral.
    - Lazy: An RDD will not actually be evaluated until it is needed for some operation.
    - Ephemeral: RDDs are discarded from memory if not needed anymore.
        - It is possible to alter the persistence level.

- Creating an RDD:
    1. parallelize an existing collection.
    2. transform another rdd.
    3. use a textfile (of format csv).
        rdd3 = sc.textFile('/path/textfile.txt')
    4. Use an existing dataframe or dataset.

- The LINEAGE of an RDD refers to all the transformations that were made to obtain the RDD.
    - lineage can be found using method .toDebugString

- Why do we use RDDs:
    - handle unstructured data.
    - they are schema-less.
    - Good for manipulating data in functional programming.
    - Memory efficient.

- DataFrames:
    - immutable, distributed collections of data.
    - unlike an RDD, dataFrames have schema.
    - DataFrames can be queried with Spark SQL.
    - Allows users to organize and structure very large sets of data.

- DataSets:
    - immutable, distributed collections of data.
    - just like DataFrames they can be queried.
    - DataSets are Strongly Typed.
    - any errors that come about as a result of dataset operations occur during Compilation, not runtime.

    As of Spark 2.0 dataSets and DataFrames are unified types.
    - DataFrame = DataSet[rows]

- Transformations vs Actions
    - Transformations:
        Operations on RDDs that return another RDD, DataFrame, or DataSet.
        Transformations are lazily evaluated.
        Transformations can be 'Narrow' or 'Wide'
            - Narrow:
                The transformation does not require every partition of the RDD to operate.
                .filter, .map
                These operate on each partition of an RDD individually.
            - Wide:
                The transformation requires all partitions of the RDD.
                .groupByKey(), reduceByKey()
                These are also "shuffle" transformations
                All Tuples with the same key must be in the same place
    - Actions:
        Operations that do not return another RDD, DataFrame, or DataSet.
        Returning a value, printing to the console, writing to a file.
        Actions are not lazily evaluated, they are performed as soon as the Spark job calls it.

- Paired RDDs
    - These are RDDs that contain key-value pairs.
    - Transformations:
        groupByKey()
        groupByKey().mapValues()
        reduceByKey()
        join()
        filter()
    - Actions:
        countByKey()
        collectAsMap()
        lookup('key')

- Good for storing information from files.
    rdd = sc.textFile('path')

- Shared Variables
    - Spark copies the variables to each node of the cluster in order for the executor to do it's job
    - This gives Spark the abililty to perform parallel processing.
    - There are 2 main types of Shared Variables

    - Shared Variables have an attribute 'value' that allows you to grab specific information

    1. Broadcast Variables
        - used to save a copy of data across all nodes.
        - that data is then cached on every machine.
        Ex.
        words = sc.broadcast(["Jacob","Hello","World"])
        print(words.value)
        print(words.value[1])

    2. Accumulators
        - used for aggregating the information of the variable.
        - useful for finding counters or sums.
        Ex.
        num = sc.accumulator(10)
        
DataFrames:
- They are another core API of Spark.
- DataFrames act like tables that can be queried like RDBMS.
- Unlike RDDs, DataFrames have a schema.
- Allows you to transform semi- and quasi-structured data into structured data.
- Create DataFrame:
    1. df = spark.createDataFrame()
    2. df = rdd.toDF()
    3. df = spark.read.csv('')

- StructType
    - Allows you to create custom schema for DataFrames
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType
    data = [("James","Smith","",23456,"M",30000),
            ("Michael","","Rose", 44342,"M",40000),
            ("Maria","Anne","Jones",44665,"F",40000),
            ("Jen","Mary","Brown",85723,"F",-1)]

    schema = StructType([ \
        StructField("firstname",StringType(),True), \
        StructField("middlename",StringType(),True), \
        StructField("lastname",StringType(),True), \
        StructField("emp_ID",IntegerType(),True), \
        StructField("gender",StringType(),True), \
        StructField("salary",IntegerType(),True)])

    - StructTypes allow you to create nested schema.

    nesteddata = [(("James","Smith",""),23456,"M",30000),
            (("Michael","","Rose"), 44342,"M",40000),
            (("Maria","Anne","Jones"),44665,"F",40000),
            (("Jen","Mary","Brown"),85723,"F",-1)]

    nestedschema = StructType([ \
        StructField("name",StructType([ \
            StructField("firstname",StringType(),True), \
            StructField("middlename",StringType(),True), \
            StructField("lastname",StringType(),True)])), \
        StructField("emp_ID",IntegerType(),True), \
        StructField("gender",StringType(),True), \
        StructField("salary",IntegerType(),True)])

- Querying DataFrames:
    .select()
    .where()
    .orderBy()
    .groupBy()

- Joins:
    dfemp.join(dfdept, dfemp.Dept == dfdept.Dept, "inner").show()

    Example of Group By Aggregation:
    dfbank.select("TX_Amount","Receiver").groupby("Receiver").agg({"TX_Amount":"sum"}).show()

DataSets
- It is a collection of data that shares some of the advantages of RDDS, but can be queried like DataFrames.
- It is strongly typed.
- A DataSet is a DataFrame organized into columns.

    In Scala, a DataFrame is a type alias of DataSet
    - DataFrame = DataSet[rows]
    - This means most of the same methods apply to both APIs
    - usually created with '.toDS()' method
    - also created with '.as[class]' where 'class' is a Case Class.

    pyspark does not support DataSets, but instead offers many DataSet capabilities in DataFrames.


RDDs vs DataFrames vs DataSets

                   |    RDD              |   DataFrame        |  DataSet
-------------------+---------------------+--------------------+---------------------
Representation     |   distributed       |  distributed       | same as DF,
of Data:           |   collection        |  collection,       | but strongly typed
                   |                     |  organized into    | 
                   |                     |  columns           |
-------------------+---------------------+--------------------+---------------------
Optimization:      |   no built-in       |  Catalyst          | Catalyst
                   |   optimization      |  Optimizer         | Optimizer
-------------------+---------------------+--------------------+---------------------
Schema:            |   no schema         |  Automatically     | Automatically
                   |                     |  finds schema      | finds schema
-------------------+---------------------+--------------------+---------------------
Aggregation:       |   slower processing |  Very quick,       | Quicker than RDD,
                   |   than DF or DS     |  Faster than RDDs  | Slower than DF
                   |                     |  and DS            |
-------------------+---------------------+--------------------+---------------------
Pyspark Support?   |   yes               |  yes               | no
-------------------+---------------------+--------------------+---------------------
Mutable/Immutable  |   Immutable         |  Immutable         | Immutable



Contexts:
- An entry point to Spark functionalities.
    1. SparkContext - has all functionalities of Apache Spark and its APIs
    2. SQLContext - has the functionalities to query and process APIS
    3. HiveContext - is SQLContext, but allows for interfacing with Hive tables
    4. StreamingContext - Entry point into Streaming Functionalities
            - Streamed data is saved as DStreams, collections of RDDs

    SparkSession: Is a single runtime of a SparkContext.
    Creating a new SparkSession will instantiate a new SparkContext.
    As of Spark 2.x, SparkSession is the entry point for SparkContext.

Spark SQL Documentation:
https://spark.apache.org/docs/latest/sql-programming-guide.html

Spark also supports Partitioning / Bucketing
- Partitioning means splitting a table into separate tables based on a column's value.
- Bucketing splits the table information into roughly equal size files.
- These only work on data that persists.

    df.write.option('path','/the/path').saveAsTable()

    df.write.partitionBy('column name').format("parquet").save("")
    df.write.bucketBy(10,'columnName').sortBy('position').saveAsTable("")

    .repartition()
        - increase or decrease the number of partitions.
    .coalesce()
        - decrease the number of partitions.
        - does repartition's job faster and more efficiently. (if you are decreasing)

Hive vs Spark?
- Spark has in-memory computation, everything is loaded in RAM
- Spark has the lazy eveluation of RDDs, meaning it's memory efficient
- Spark has real-time analytics on Streamed data
- Hive is better for long-term storage
- Hive is overall faster at accessing very large sets of data (tens/hundreds of thousands of records)

Spark-Submit
- a command that allows you to submit Spark jobs without needing to re-compile them
- In Scala you would spark-submit a .jar file, with pyspark, you can submit a .py file

- Spark-Submit Options:

    ./bin/spark-submit \
        --master <master-url> \
        --deploy-mode <deploy-mode> \
        --conf <key<=<value> \
        --driver-memory <value>g \
        --executor-memory <value>g \
        --executor-cores <number of cores>  \
        --py-files file1.py,file2.py,file3.zip, file4.egg \
        wordByExample.py [application-arguments]

    - Spark-Submit Options Guide:
        https://sparkbyexamples.com/pyspark/spark-submit-python-file/#:~:text=1.%20Spark%20Submit%20Python%20File%20%20%20,binary%20executable%20to%20use%20for%20PySp%20...%20

